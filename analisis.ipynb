{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "def leer_columnas(archivo_csv, c_names, c_labels, c_phase, phase):\n",
    "    try:\n",
    "        df = pd.read_csv(archivo_csv, sep=\" \", header=None)\n",
    "        if phase:\n",
    "            df = df[[c_names, c_labels, c_phase]]\n",
    "            df.columns = [\"ID\", \"Label\", \"Phase\"]\n",
    "        else:\n",
    "            df = df[[c_names, c_labels]]\n",
    "            df.columns = [\"ID\", \"Label\"]\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"El archivo de metadatos no existe\")\n",
    "    except KeyError:\n",
    "        print(\"Las columnas especificadas no existen en el archivo de metadatos\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(score_file, metadata='Scores/LA/trial_metadata.txt', col_names=1, col_labels=5, phase='eval', col_phase=7):\n",
    "    metadatos = leer_columnas(metadata, col_names, col_labels, col_phase, phase)\n",
    "    scores = pd.read_csv(score_file, sep=\" \", header=None, skipinitialspace=True)\n",
    "    scores.columns = [\"ID\", \"Scores\"]\n",
    "    if len(scores) != len(metadatos):\n",
    "        print(\"CHECK: submission has %d of %d expected trials.\" % (len(scores), len(metadatos)))\n",
    "    if len(scores.columns) > 2:\n",
    "        print(\"CHECK: submission has more columns (%d) than expected (2). Check for leading/ending blank spaces.\"\n",
    "              % len(scores.columns)\n",
    "              )\n",
    "        exit(1)\n",
    "    if phase:\n",
    "        cm_scores = scores.merge(metadatos[metadatos[\"Phase\"] == phase], on=\"ID\")\n",
    "    else:\n",
    "        cm_scores = scores.merge(metadatos, on=\"ID\")\n",
    "    return cm_scores\n",
    "\n",
    "def extract_eer(score_file, metadata='Scores/LA/trial_metadata.txt', col_names=1, col_labels=5, phase='progress', col_phase=7, pri=True):\n",
    "    cm_scores= merge(score_file, metadata, col_names, col_labels, phase, col_phase)\n",
    "    bona_cm = cm_scores[cm_scores[\"Label\"] == \"bonafide\"][\"Scores\"].values\n",
    "    spoof_cm = cm_scores[cm_scores[\"Label\"] == \"spoof\"][\"Scores\"].values\n",
    "    eer_cm, threshold = compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"EER: {:.4f}. Threshold: {:.5f}\\n\".format(100 * eer_cm, threshold)\n",
    "    if pri: print(out_data)\n",
    "    return eer_cm, threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Anti-spoofing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQCC_GMM\n",
      "EER: 11.3095. Threshold: 0.73193\n",
      "\n",
      "LFCC_GMM\n",
      "EER: 7.2918. Threshold: 0.80453\n",
      "\n",
      "LFCC_LCNN\n",
      "CHECK: submission has 24986 of 24844 expected trials.\n",
      "EER: 0.5506. Threshold: -5.17534\n",
      "\n",
      "RawNet2\n",
      "EER: 1.2957. Threshold: -7.05358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "thresholds=[]\n",
    "eers=[]\n",
    "Database = 'ASVspoof2019_LA_dev'\n",
    "for model in Models:\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/LA', Database, model + '.txt')\n",
    "    eer, th = extract_eer(score_file = score_path, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=4, phase=None)\n",
    "    thresholds.append(th)\n",
    "    eers.append(eer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQCC_GMM\n",
      "EER: 15.6181. Threshold: 0.79618\n",
      "\n",
      "LFCC_GMM\n",
      "EER: 19.2967. Threshold: 1.15404\n",
      "\n",
      "LFCC_LCNN\n",
      "EER: 9.2601. Threshold: 5.26733\n",
      "\n",
      "RawNet2\n",
      "EER: 7.4716. Threshold: -5.78430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "thresholds=[]\n",
    "eers=[]\n",
    "Database = 'ASVspoof2021_LA_eval'\n",
    "for model in Models:\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/LA', Database, model + '.txt')\n",
    "    eer, th = extract_eer(score_file = score_path, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='eval')\n",
    "    thresholds.append(th)\n",
    "    eers.append(eer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with thresholds, we can see which bonafides are classified as spoof and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_fallos = []\n",
    "Database = 'ASVspoof2021_LA_eval'\n",
    "for i in range(0, len(Models)):\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[i] + '.txt')\n",
    "    df_merged = merge(score_path, metadata='Metadata/' + Database + '/trial_metadata.txt', col_names=1, col_labels=5, phase='eval', col_phase=7)\n",
    "    df_merged\n",
    "\n",
    "    condition_1 = (df_merged['Label'] == 'bonafide') & (df_merged['Scores'] < thresholds[i])\n",
    "    condition_2 = (df_merged['Label'] == 'spoof') & (df_merged['Scores'] > thresholds[i])\n",
    "\n",
    "    selected_rows = df_merged[condition_1 | condition_2]\n",
    "    dfs_fallos.append(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CQCC_GMM  LFCC_GMM  LFCC_LCNN  RawNet2\n",
      "CQCC_GMM      23142     11231       7340     4314\n",
      "LFCC_GMM      11231     28593       3688     2501\n",
      "LFCC_LCNN      7340      3688      13721     4678\n",
      "RawNet2        4314      2501       4678    11071\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Bucles for anidados para filas y columnas\n",
    "for i in range(len(Models)):  # Bucle externo para filas\n",
    "    df_fila = dfs_fallos[i]\n",
    "    for j in range(len(Models)):  # Bucle interno para columnas\n",
    "        df_columna = dfs_fallos[j]\n",
    "        # Calcular el número que deseas añadir al DataFrame\n",
    "        fallos_comun = df_fila.merge(df_columna, on=\"ID\")\n",
    "        \n",
    "        # Añadir el número al DataFrame\n",
    "        df.at[i, j] = len(fallos_comun)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df = df.astype(int)\n",
    "df.rename(index=dict(zip(df.index, Models)), columns=dict(zip(df.index, Models)), inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallan_todos = dfs_fallos[0][\"ID\"]\n",
    "for i in range(len(Models)):  # Bucle externo para filas\n",
    "    df_fila = dfs_fallos[i]\n",
    "\n",
    "    fallan_todos = df_fila.merge(fallan_todos, on=\"ID\")[\"ID\"]\n",
    "        \n",
    "len(fallan_todos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores normalization for classic ensemble model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was used to generate the normalized scores referenced in the paper. While it is included for transparency, running it is not required, as both the raw and normalized scores are available in the 'Scores' folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "Models=['ModeloClasificacion']\n",
    "def sigmoid_transform(score):\n",
    "    return 1 / (1 + np.exp(-score))\n",
    "def min_max_scaling(score, min_val, max_val):\n",
    "    return (score - min_val) / (max_val - min_val)\n",
    "\n",
    "for i in range(len(Models)):\n",
    "    score_file=os.path.join('Scores/LA', 'ASVspoof2021_LA_eval', Models[i] + '.txt') # Change the Name of the dataset to normalize other datasets scores\n",
    "    scores = pd.read_csv(score_file, sep=\" \", header=None, skipinitialspace=True)\n",
    "    min_score = scores[1].min()\n",
    "    max_score = scores[1].max()\n",
    "    scores[1] = scores[1].apply(lambda x: min_max_scaling(x, min_score, max_score))\n",
    "    scores[1] = scores[1].apply(sigmoid_transform)\n",
    "    score_file_norm=os.path.join('Scores/LA', 'ASVspoof2021_LA_eval', Models[i] + '_norm.txt') # Change the Name of the dataset to normalize other datasets scores\n",
    "    scores.to_csv(path_or_buf=score_file_norm, sep=' ', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Based (EB) ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal#\n",
      "CQCC_GMM\n",
      "EER: 17.6317. Threshold: 0.70615\n",
      "\n",
      "LFCC_GMM\n",
      "EER: 21.0128. Threshold: 0.72422\n",
      "\n",
      "LFCC_LCNN\n",
      "EER: 11.6129. Threshold: 0.70594\n",
      "\n",
      "RawNet2\n",
      "EER: 5.7913. Threshold: 0.73072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "thresholds_norm=[]\n",
    "eers_norm=[]\n",
    "Database = 'ASVspoof2021_DF_eval'\n",
    "print('#Normal#')\n",
    "for model in Models:\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/DF', Database, model + '_norm.txt')\n",
    "    eer, th = extract_eer(score_file = score_path, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress')\n",
    "    thresholds_norm.append(th)\n",
    "    eers_norm.append(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Modelo Fusion Clasico#\n",
      "CQCC_GMM\n",
      "LFCC_GMM\n",
      "LFCC_LCNN\n",
      "RawNet2\n"
     ]
    }
   ],
   "source": [
    "def weighted(score, weight):\n",
    "    return (score) * weight\n",
    "\n",
    "Database='ASVspoof2021_DF_eval'\n",
    "print('#Modelo Fusion Clasico#')\n",
    "score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "print(Models[0])\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "scores[1] = scores[1].apply(lambda x: weighted(x, eers_norm[0]))\n",
    "modelo_fusion=scores\n",
    "\n",
    "for i,model in enumerate(Models[1:4]):\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/DF', Database, model + '_norm.txt')\n",
    "    scores_new = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    modelo_fusion[1]=modelo_fusion[1] + scores_new[1].apply(lambda x: weighted(x, eers_norm[i]))\n",
    "\n",
    "#score_file_mf = os.path.join('Scores/DF', Database, 'EB_Ensemble.txt')\n",
    "#modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 6.6414. Threshold: 0.47428\n",
      "\n",
      "EER: 20.1504. Threshold: 0.45982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_file_mf = os.path.join('Scores/LA', 'ASVspoof2021_LA_eval','EB_Ensemble.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', 'ASVspoof2021_LA_eval', 'trial_metadata.txt'), col_labels=5, phase='eval')\n",
    "\n",
    "score_file_mf = os.path.join('Scores/DF', 'ASVspoof2021_DF_eval','EB_Ensemble.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', 'ASVspoof2021_DF_eval', 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 baseline models + clasification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal#\n",
      "CQCC_GMM\n",
      "EER: 17.6317. Threshold: 0.70615\n",
      "\n",
      "LFCC_GMM\n",
      "EER: 21.0128. Threshold: 0.72422\n",
      "\n",
      "LFCC_LCNN\n",
      "EER: 11.6129. Threshold: 0.70594\n",
      "\n",
      "RawNet2\n",
      "EER: 5.7913. Threshold: 0.73072\n",
      "\n",
      "ModeloClasificacion\n",
      "EER: 3.3801. Threshold: 0.58629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2', 'ModeloClasificacion']\n",
    "thresholds_norm=[]\n",
    "eers_norm=[]\n",
    "Database = 'ASVspoof2021_DF_eval'\n",
    "print('#Normal#')\n",
    "for model in Models:\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/DF', Database, model + '_norm.txt')\n",
    "    eer, th = extract_eer(score_file = score_path, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress')\n",
    "    thresholds_norm.append(th)\n",
    "    eers_norm.append(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQCC_GMM\n",
      "LFCC_GMM\n",
      "LFCC_LCNN\n",
      "RawNet2\n"
     ]
    }
   ],
   "source": [
    "def weighted(score, weight):\n",
    "    return (score) * weight\n",
    "\n",
    "Database='ASVspoof2021_DF_eval'\n",
    "score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "print(Models[0])\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "scores[1] = scores[1].apply(lambda x: weighted(x, eers_norm[0]))\n",
    "modelo_fusion=scores\n",
    "\n",
    "for i,model in enumerate(Models[1:5]):\n",
    "    print(model)\n",
    "    score_path=os.path.join('Scores/DF', Database, model + '_norm.txt')\n",
    "    scores_new = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    modelo_fusion[1]=modelo_fusion[1] + scores_new[1].apply(lambda x: weighted(x, eers_norm[i]))\n",
    "\n",
    "#score_file_mf = os.path.join('Scores/DF', Database, 'EB_Ensemble_5_models.txt')\n",
    "#modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 5.7566. Threshold: 0.46809\n",
      "\n",
      "EER: 12.9785. Threshold: 0.49408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_file_mf = os.path.join('Scores/LA', 'ASVspoof2021_LA_eval','EB_Ensemble_5_models.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', 'ASVspoof2021_LA_eval', 'trial_metadata.txt'), col_labels=5, phase='eval')\n",
    "\n",
    "score_file_mf = os.path.join('Scores/DF', 'ASVspoof2021_DF_eval','EB_Ensemble_5_models.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', 'ASVspoof2021_DF_eval', 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.7)\n",
      "Mejor EER de progress: 0.06979769121967563\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.2, 0.6)\n",
      "Mejor EER de progress: 0.06916124988137837\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.25, 0.55)\n",
      "Mejor EER de progress: 0.06869386453442544\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.3, 0.5)\n",
      "Mejor EER de progress: 0.0686262421378877\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.35, 0.45)\n",
      "Mejor EER de progress: 0.06812504559266591\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.4, 0.4)\n",
      "Mejor EER de progress: 0.06631715517258074\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.45, 0.35)\n",
      "Mejor EER de progress: 0.06504427249598624\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.5, 0.3)\n",
      "Mejor EER de progress: 0.06327019327416991\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.55, 0.25)\n",
      "Mejor EER de progress: 0.06018942017749023\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.6, 0.2)\n",
      "Mejor EER de progress: 0.057176269477348274\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.2, 0.6, 0.1)\n",
      "Mejor EER de progress: 0.0501137637883519\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.25, 0.55, 0.1)\n",
      "Mejor EER de progress: 0.04951113364832352\n",
      "Nueva mejor combinación:\n",
      "(0.2, 0.2, 0.5, 0.1)\n",
      "Mejor EER de progress: 0.04887469231002625\n",
      "Nueva mejor combinación:\n",
      "(0.2, 0.25, 0.45, 0.1)\n",
      "Mejor EER de progress: 0.048441118161342195\n",
      "Nueva mejor combinación:\n",
      "(0.25, 0.2, 0.45, 0.1)\n",
      "Mejor EER de progress: 0.04837349576480446\n",
      "Nueva mejor combinación:\n",
      "(0.3, 0.2, 0.4, 0.1)\n",
      "Mejor EER de progress: 0.047770865624776074\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "import itertools\n",
    "pesos = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7]\n",
    "\n",
    "# Generar todas las combinaciones posibles de 4 pesos que sumen 1\n",
    "combinaciones = itertools.product(pesos, repeat=4)\n",
    "\n",
    "# Filtrar las combinaciones que sumen 1\n",
    "combinaciones_validas = filter(lambda x: sum(x) == 1, combinaciones)\n",
    "# Iterar sobre las combinaciones válidas\n",
    "Database = 'ASVspoof2021_LA_eval'\n",
    "score_path=os.path.join('Scores/LA', Database, Models[0] + '_norm.txt')\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "modelo_fusion=scores\n",
    "mejor_err=100\n",
    "for combinacion in combinaciones_validas:\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[0] + '_norm.txt')\n",
    "    scores0 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[1] + '_norm.txt')\n",
    "    scores1 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[2] + '_norm.txt')\n",
    "    scores2 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[3] + '_norm.txt')\n",
    "    scores3 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    p1,p2,p3,p4=combinacion\n",
    "    modelo_fusion[1] = scores0[1].apply(\n",
    "        lambda x: weighted(x, p1)) + scores1[1].apply(\n",
    "        lambda x: weighted(x, p2)) + scores2[1].apply(\n",
    "        lambda x: weighted(x, p3)) + scores3[1].apply(\n",
    "        lambda x: weighted(x, p4))\n",
    "    score_file_mf = os.path.join('Scores/LA', Database, '_Modelo_Fusion.txt')\n",
    "    modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "    eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress', pri=False)\n",
    "    if eer<mejor_err:\n",
    "        mejor_err=eer\n",
    "        score_file_mff = os.path.join('Scores/LA', Database, 'GS_Ensemble.txt')\n",
    "        modelo_fusion.to_csv(path_or_buf=score_file_mff, sep=' ', header=False, index=False)\n",
    "        mejor_combinacion=combinacion\n",
    "        print('Nueva mejor combinación:')\n",
    "        print(combinacion)\n",
    "        print('Mejor EER de progress: {}'.format(eer) )\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 5.7843. Threshold: 0.70215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Database = 'ASVspoof2021_LA_eval'\n",
    "score_file_mff = os.path.join('Scores/LA', Database, 'GS_Ensemble.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mff, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 baseline models + clasification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.1, 0.6)\n",
      "Mejor EER de progress: 0.055503623850338565\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.15, 0.1, 0.55)\n",
      "Mejor EER de progress: 0.054799560115503576\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.25, 0.1, 0.45)\n",
      "Mejor EER de progress: 0.05429836357028178\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.3, 0.1, 0.4)\n",
      "Mejor EER de progress: 0.052625717943272074\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.35, 0.1, 0.35)\n",
      "Mejor EER de progress: 0.052056899001512544\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.4, 0.1, 0.3)\n",
      "Mejor EER de progress: 0.05185403181189935\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.45, 0.1, 0.25)\n",
      "Mejor EER de progress: 0.05075020512664916\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.5, 0.1, 0.2)\n",
      "Mejor EER de progress: 0.04957875604486125\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.5, 0.15, 0.15)\n",
      "Mejor EER de progress: 0.04894231470656399\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.55, 0.1, 0.15)\n",
      "Mejor EER de progress: 0.04529272266812477\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.6, 0.1, 0.1)\n",
      "Mejor EER de progress: 0.042245760769713946\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2', 'ModeloClasificacion']\n",
    "import itertools\n",
    "pesos = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]\n",
    "\n",
    "# Generar todas las combinaciones posibles de 4 pesos que sumen 1\n",
    "combinaciones = itertools.product(pesos, repeat=5)\n",
    "\n",
    "# Filtrar las combinaciones que sumen 1\n",
    "combinaciones_validas = filter(lambda x: sum(x) == 1, combinaciones)\n",
    "# Iterar sobre las combinaciones válidas\n",
    "Database = 'ASVspoof2021_LA_eval'\n",
    "score_path=os.path.join('Scores/LA', Database, Models[0] + '_norm.txt')\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "modelo_fusion=scores\n",
    "mejor_err=100\n",
    "for combinacion in combinaciones_validas:\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[0] + '_norm.txt')\n",
    "    scores0 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[1] + '_norm.txt')\n",
    "    scores1 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[2] + '_norm.txt')\n",
    "    scores2 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[3] + '_norm.txt')\n",
    "    scores3 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/LA', Database, Models[4] + '_norm.txt')\n",
    "    scores4 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    p1,p2,p3,p4,p5=combinacion\n",
    "    modelo_fusion[1] = scores0[1].apply(\n",
    "        lambda x: weighted(x, p1)) + scores1[1].apply(\n",
    "        lambda x: weighted(x, p2)) + scores2[1].apply(\n",
    "        lambda x: weighted(x, p3)) + scores3[1].apply(\n",
    "        lambda x: weighted(x, p4)) + scores4[1].apply(\n",
    "        lambda x: weighted(x, p5))\n",
    "    score_file_mf = os.path.join('Scores/LA', Database, '_Modelo_Fusion.txt')\n",
    "    modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "    eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress', pri=False)\n",
    "    if eer<mejor_err:\n",
    "        mejor_err=eer\n",
    "        score_file_mff = os.path.join('Scores/LA', Database, 'GS_Ensemble_5.txt')\n",
    "        modelo_fusion.to_csv(path_or_buf=score_file_mff, sep=' ', header=False, index=False)\n",
    "        mejor_combinacion=combinacion\n",
    "        print('Nueva mejor combinación:')\n",
    "        print(combinacion)\n",
    "        print('Mejor EER de progress: {}'.format(eer) )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 5.2095. Threshold: 0.68692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Database = 'ASVspoof2021_LA_eval'\n",
    "score_file_mff = os.path.join('Scores/LA', Database, 'GS_Ensemble_5.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mff, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.7)\n",
      "Mejor EER de progress: 0.07056102870891026\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2']\n",
    "import itertools\n",
    "pesos = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7]\n",
    "combinaciones = itertools.product(pesos, repeat=4)\n",
    "# Filtrar las combinaciones que sumen 1\n",
    "combinaciones_validas = filter(lambda x: sum(x) == 1, combinaciones)\n",
    "\n",
    "# Iterar sobre las combinaciones válidas\n",
    "Database = 'ASVspoof2021_DF_eval'\n",
    "score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "modelo_fusion=scores\n",
    "mejor_err=100\n",
    "for combinacion in combinaciones_validas:\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "    scores0 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[1] + '_norm.txt')\n",
    "    scores1 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[2] + '_norm.txt')\n",
    "    scores2 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[3] + '_norm.txt')\n",
    "    scores3 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    p1,p2,p3,p4=combinacion\n",
    "    modelo_fusion[1] = scores0[1].apply(\n",
    "        lambda x: weighted(x, p1)) + scores1[1].apply(\n",
    "        lambda x: weighted(x, p2)) + scores2[1].apply(\n",
    "        lambda x: weighted(x, p3)) + scores3[1].apply(\n",
    "        lambda x: weighted(x, p4))\n",
    "    score_file_mf = os.path.join('Scores/DF', Database, '_Modelo_Fusion.txt')\n",
    "    modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "    eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress', pri=False)\n",
    "    if eer<mejor_err:\n",
    "        mejor_err=eer\n",
    "        score_file_mff = os.path.join('Scores/DF', Database, 'GS_Ensemble.txt')\n",
    "        modelo_fusion.to_csv(path_or_buf=score_file_mff, sep=' ', header=False, index=False)\n",
    "        mejor_combinacion=combinacion\n",
    "        print('Nueva mejor combinación:')\n",
    "        print(combinacion)\n",
    "        print('Mejor EER de progress: {}'.format(eer) )\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 23.6331. Threshold: 0.58832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Database = 'ASVspoof2021_DF_eval'\n",
    "score_file_mff = os.path.join('Scores/DF', Database, 'GS_Ensemble.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mff, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 baseline models + clasification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.1, 0.6)\n",
      "Mejor EER de progress: 0.024641947577492523\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.15, 0.55)\n",
      "Mejor EER de progress: 0.01993947878052437\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.2, 0.5)\n",
      "Mejor EER de progress: 0.01735085439322337\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.25, 0.45)\n",
      "Mejor EER de progress: 0.014734222462557359\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.3, 0.4)\n",
      "Mejor EER de progress: 0.011273387431701021\n",
      "Nueva mejor combinación:\n",
      "(0.1, 0.1, 0.1, 0.35, 0.35)\n",
      "Mejor EER de progress: 0.010571884577741417\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Models=['CQCC_GMM', 'LFCC_GMM', 'LFCC_LCNN', 'RawNet2', 'ModeloClasificacion']\n",
    "import itertools\n",
    "pesos = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]\n",
    "combinaciones = itertools.product(pesos, repeat=5)\n",
    "# Filtrar las combinaciones que sumen 1\n",
    "combinaciones_validas = filter(lambda x: sum(x) == 1, combinaciones)\n",
    "\n",
    "# Iterar sobre las combinaciones válidas\n",
    "Database = 'ASVspoof2021_DF_eval'\n",
    "score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "scores = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "modelo_fusion=scores\n",
    "mejor_err=100\n",
    "for combinacion in combinaciones_validas:\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[0] + '_norm.txt')\n",
    "    scores0 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[1] + '_norm.txt')\n",
    "    scores1 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[2] + '_norm.txt')\n",
    "    scores2 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[3] + '_norm.txt')\n",
    "    scores3 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    score_path=os.path.join('Scores/DF', Database, Models[4] + '_norm.txt')\n",
    "    scores4 = pd.read_csv(score_path, sep=\" \", header=None, skipinitialspace=True)\n",
    "    p1,p2,p3,p4,p5=combinacion\n",
    "    modelo_fusion[1] = scores0[1].apply(\n",
    "        lambda x: weighted(x, p1)) + scores1[1].apply(\n",
    "        lambda x: weighted(x, p2)) + scores2[1].apply(\n",
    "        lambda x: weighted(x, p3)) + scores3[1].apply(\n",
    "        lambda x: weighted(x, p4)) + scores4[1].apply(\n",
    "        lambda x: weighted(x, p5))\n",
    "    score_file_mf = os.path.join('Scores/DF', Database, '_Modelo_Fusion.txt')\n",
    "    modelo_fusion.to_csv(path_or_buf=score_file_mf, sep=' ', header=False, index=False)\n",
    "    eer, th = extract_eer(score_file = score_file_mf, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='progress', pri=False)\n",
    "    if eer<mejor_err:\n",
    "        mejor_err=eer\n",
    "        score_file_mff = os.path.join('Scores/DF', Database, 'GS_Ensemble_5.txt')\n",
    "        modelo_fusion.to_csv(path_or_buf=score_file_mff, sep=' ', header=False, index=False)\n",
    "        mejor_combinacion=combinacion\n",
    "        print('Nueva mejor combinación:')\n",
    "        print(combinacion)\n",
    "        print('Mejor EER de progress: {}'.format(eer) )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 11.3928. Threshold: 0.60410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Database = 'ASVspoof2021_DF_eval'\n",
    "score_file_mff = os.path.join('Scores/DF', Database, 'GS_Ensemble_5.txt')\n",
    "eer, th = extract_eer(score_file = score_file_mff, metadata=os.path.join('Metadata', Database, 'trial_metadata.txt'), col_labels=5, phase='eval')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech_Antispoofing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
